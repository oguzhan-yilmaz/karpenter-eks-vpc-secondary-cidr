{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EKS Cluster w/ Secondary CIDR and Karpenter Configuration","text":""},{"location":"#index","title":"Index","text":"<ul> <li>Demo: AWS VPC CNI Custom Networking<ul> <li>About the Demo</li> <li>1. Create VPC with Secondary CIDR and Subnets</li> <li>2. AWS VPC CNI &amp; ENIConfig configuration for Custom Networking</li> <li>3. Karpenter v1alpha Configuration</li> <li>Demo Troubleshooting</li> </ul> </li> <li>ec2-instance-selector CLI tool</li> </ul>"},{"location":"#eni-custom-networking-demo","title":"ENI Custom Networking Demo","text":"<ul> <li>Creates an EKS Cluster with a VPC with Secondary CIDR block.<ul> <li>Secondary CIDR block is a VPC feature that allows you to add additional IP addresses to your VPC.</li> <li>We want to use the secondary CIDR block for the pods, and the default CIDR block of the VPC for the nodes.</li> <li>Thus defeating the IP Exhaustion problem.</li> </ul> </li> <li>Creates 3 Private subnets in the Secondary CIDR block with <code>/19</code> mask, so we can have available IP count of <code>3*8190</code> or <code>24570</code> for our pods.</li> <li>Updates <code>aws-node</code> with Custom Networking configuration.</li> <li>Creates ENIConfig for each of our subnets in the Secondary CIDR block.</li> <li>Creates Karpenter Provisioner and AWSNodeTemplate.</li> <li>Offers troubleshooting steps for common issues.</li> <li>Recommends how to choose EC2 Instance Types. </li> </ul>"},{"location":"#demo-diagram","title":"Demo Diagram","text":""},{"location":"checks/","title":"Sanity Checks","text":""},{"location":"checks/#-eniconfig","title":"- EniConfig","text":""},{"location":"checks/#todo","title":"TODO","text":""},{"location":"checks/#todo_1","title":"TODO","text":""},{"location":"checks/#todo_2","title":"TODO","text":""},{"location":"ec2-instance-selector/","title":"AWS ec2-instance-selector CLI tool","text":"<ul> <li>AWS Docs: Instance Types</li> <li>Karpenter uses the price-capacity-optimized strategy. EC2 Fleet identifies the pools with the highest capacity availability for the number of instances that are launching. </li> <li>This means that we will request Spot Instances from the pools that we believe have the lowest chance of interruption in the near term. </li> <li>EC2 Fleet then requests Spot Instances from the lowest priced of these pools.</li> <li>Knowing this, it's important to give Karpenter the widest possible list of EC2 Instance Types.</li> </ul>"},{"location":"ec2-instance-selector/#installation","title":"Installation","text":"<ul> <li>Visit ec2-instance-selector Github Repo</li> </ul>"},{"location":"ec2-instance-selector/#aws-instance-types-summary","title":"AWS Instance Types Summary","text":"EC2 Instance Type Optimized For m5.large General purpose c5.large Compute optimized r5.large Memory optimized g4dn.xlarge GPU optimized"},{"location":"ec2-instance-selector/#example-cli-commands","title":"Example CLI Commands","text":"<p>Export your AWS region.</p> <pre><code>export AWS_REGION=\"eu-central-1\"\n</code></pre>"},{"location":"ec2-instance-selector/#list-as-table","title":"List as table","text":"<pre><code>ec2-instance-selector \\\n    --memory 4 \\\n    --vcpus 2 \\\n    -o table-wide \\\n    --cpu-architecture x86_64/amd64\n</code></pre>"},{"location":"ec2-instance-selector/#interactive-mode","title":"Interactive mode","text":"<pre><code>ec2-instance-selector \\\n    --memory-min 2 \\\n    --memory-max 8 \\\n    --vcpus-min 2 \\\n    --vcpus-max 6 \\\n    --gpus 0 \\\n    --usage-class spot \\\n    --network-interfaces-min 2 \\\n    --disk-type ssd \\\n    --cpu-architecture \"x86_64\" \\\n    -o interactive\n</code></pre>"},{"location":"karpenter-v1beta-configuration/","title":"Demo: Karpenter v1beta","text":"<p>https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh</p> <ul> <li>karpenter.k8s.aws/instance-pods https://karpenter.sh/docs/reference/instance-types/</li> <li>check this node label!</li> <li>https://github.com/awslabs/amazon-eks-ami/blob/master/log-collector-script/linux/README.md</li> <li>collect logs of userdata + kubelet???</li> </ul>"},{"location":"karpenter-v1beta-configuration/#installation","title":"Installation","text":"Export env. variables we will use in this demo<pre><code>export KARPENTER_VERSION=v0.32.1\nexport K8S_VERSION=1.24\nexport AWS_PAGER=\"\"                          # disable the aws cli pager\nexport AWS_PROFILE=hepapi\nexport AWS_REGION=eu-central-1\nexport CLUSTER_NAME=\"miyazaki\"                 # will be created with eksctl\n</code></pre>"},{"location":"karpenter-v1beta-configuration/#new-one","title":"new one","text":"Export env. variables we will use in this demo<pre><code>eksdemo create cluster \"$CLUSTER_NAME\" \\\n    --instance \"m5.large\" \\\n    --nodes 2 \\\n    --version \"$K8S_VERSION\" \\\n    --os \"AmazonLinux2\"\n\n\neksdemo install autoscaling-karpenter \\\n    --cluster \"$CLUSTER_NAME\" \\\n    --version \"$KARPENTER_VERSION\" \\\n    --set \"hostNetwork=true,\" \n</code></pre>"},{"location":"karpenter-v1beta-configuration/#create-an-eks-cluster-with-karpenter","title":"Create an EKS cluster with Karpenter","text":"<pre><code>eksctl create cluster -f - &lt;&lt;EOF\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: ${CLUSTER_NAME}\n  region: ${AWS_REGION}\n  version: \"${K8S_VERSION}\"\n  tags:\n    karpenter.sh/discovery: ${CLUSTER_NAME} #\u00a0here, it is set to the cluster name\niam:\n  withOIDC: true \n\nkarpenter:\n  version: \"${KARPENTER_VERSION}\"\n  createServiceAccount: true \n  defaultInstanceProfile: \"KarpenterNodeInstanceProfile-${CLUSTER_NAME}\"\n  withSpotInterruptionQueue: true \n\nmanagedNodeGroups:\n- instanceType: m5.large\n  amiFamily: AmazonLinux2\n  name: ${CLUSTER_NAME}-ng\n  desiredCapacity: 2\n  minSize: 1\n  maxSize: 5\nEOF\n</code></pre>"},{"location":"karpenter-v1beta-configuration/#find-all-aws-resources-tagged-with-karpentershdiscoverycluster_name","title":"Find all AWS resources tagged with <code>karpenter.sh/discovery=${CLUSTER_NAME}</code>","text":"<p>Check if you got everything tagged correctly.</p> <p>It should include the following resources (just as your original EKS NodeGroup):</p> <ul> <li>Private Subnets</li> <li>Cluster SecurityGroup</li> </ul> <pre><code># List all resources with the tag: 'karpenter.sh/discovery=${CLUSTER_NAME}'\naws resourcegroupstaggingapi get-resources \\\n    --tag-filters \"Key=karpenter.sh/discovery,Values=${CLUSTER_NAME}\" \\\n    --query 'ResourceTagMappingList[]' --output text \\\n    | sed 's/^arn:/\\n----------\\narn:/g'\n</code></pre> Check Karpenter logs<pre><code>kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter\n</code></pre> <ul> <li>Maybe set hostNetwork: true ?</li> </ul> Check interruped sqs queue<pre><code>aws sqs get-queue-attributes \\\n    --queue-url \"https://sqs.${AWS_REGION}.amazonaws.com/${ACCOUNT_ID}/${CLUSTER_NAME}\" \\\n    --attribute-names ApproximateNumberOfMessages --no-cli-pager --query 'Attributes'\n</code></pre>"},{"location":"karpenter-v1beta-configuration/#create","title":"Create","text":""},{"location":"karpenter-v1beta-configuration/#scale-up-the-cluster","title":"Scale up the cluster","text":"Scale up the cluster"},{"location":"migrate-from-awsnodetemplate-to-nodeclass/","title":"Migrate from AWSNodeTemplate to NodeClass (a.k.a. v1beta1 Migration)","text":"<p>TODO: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI  </p> <p>TODO: Remember that when changing the max pods you also should pass in --kube-reserved to the kubelet extra args as otherwise it will still use the ENI limits for the default instance pod count (see #782 for more details).</p> <p>TODO     Provisioner -&gt; NodePool     Machine -&gt; NodeClaim     AWSNodeTemplate -&gt; EC2NodeClass</p> <p>coming soon...</p>"},{"location":"migrate-from-awsnodetemplate-to-nodeclass/#plan","title":"Plan","text":"<p>Node Classes enable configuration of AWS specific settings. Each NodePool must reference an EC2NodeClass using spec.template.spec.nodeClassRef. Multiple NodePools may point to the same EC2NodeClass.</p> <p>SpotENI3NodePool     max_pods: todo calculate SpotENI2NodePool     max_pods: todo calculate OnDemandENI3NodePool     max_pods: todo calculate OnDemandENI2NodePool     max_pods: todo calculate</p>"},{"location":"multi-nodeclass-strategy/","title":"Multi nodeclass strategy","text":"<p>coming soon...</p>"},{"location":"todo/","title":"Todo","text":"<ul> <li>https://aws.amazon.com/blogs/containers/karpenter-graduates-to-beta/</li> <li>v1beta is out</li> <li>full example yaml files<ul> <li>multi nodeclass strategy<ul> <li>arm and x64 (would require two different nodeclasses)</li> <li>spot and ondemand (is just fine with one nodeclass)</li> </ul> </li> </ul> </li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/","title":"Demo: EKS Custom Networking w/ VPC Secondary CIDR block","text":"<ul> <li>This configuration keeps nodes and pods IP addresses in the different CIDR blocks.</li> <li>AWS VPCs has a default CIDR block, and you can add a secondary CIDR block to the VPC.</li> <li>We will use the secondary CIDR block for the pods, and the default CIDR block of the VPC for the nodes.</li> <li>Secondary CIDR will have 3 subnets with <code>/19</code> mask, so we can have available IP count of <code>3*8190~=24570</code> for our pods.</li> <li>This tutorial also includes karpenter configuration for make use of the secondary CIDR block.</li> <li>This demo is for pre <code>v0.32</code> or <code>v1alpha</code> Karpenter version, but should work fine for AWS CNI and ENIConfig.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#why-is-this-needed","title":"Why is this needed?","text":"<ul> <li>Running many nodes in EKS can cause IP address exhaustion in the VPC.</li> <li>How many IP addresses are available to a node is determined by nodes ENI capacity.<ul> <li>Because of this, EKS requires running many nodes to keep up with the Pod count.</li> </ul> </li> <li>Using a VPC with Secondary CIDR block allows us to have more IP addresses available to our pods.</li> <li>Karpenter is a faster option for cluster autoscaling than the default EKS Cluster Autoscaler.</li> <li>Karpenter can be configured to use Spot Instances, which can save a lot of money.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#hands-on-demo","title":"Hands-on Demo","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>jq</code></li> <li><code>eksdemo</code></li> <li><code>yq</code></li> <li><code>kubectl</code></li> <li><code>aws cli</code></li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#about-the-demo","title":"About the Demo","text":"<p>Following is a full demo that will configure CNI Custom Networking for EKS:</p> <ul> <li>create an EKS Cluster with a VPC Secondary CIDR block,</li> <li>and will configure AWS VPC CNI,</li> <li>and also the Karpenter to use the secondary CIDR block for the pods.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#index","title":"Index","text":"<ul> <li>1. Create VPC with Secondary CIDR and Subnets</li> <li>2. AWS VPC CNI &amp; ENIConfig configuration for Custom Networking   </li> <li>3. Karpenter v1alpha Configuration (Provider &amp; AWSNodeTemplate)</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/#demo-diagram","title":"Demo Diagram","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/","title":"1. Create VPC with Secondary CIDR and Subnets","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#export-variables","title":"Export Variables","text":"Export the variables we will be using throughout the demo<pre><code>export AWS_PAGER=\"\"                          # disable the aws cli pager \nexport AWS_PROFILE=hepapi\nexport AWS_REGION=eu-central-1                   \nexport CLUSTER_NAME=\"miyazaki\"                 # will be created with eksdemo tool\nexport CLUSTER_VPC_CIDR=\"194.151.0.0/16\"     # main EKS Cluster VPC CIDR\nexport SECONDARY_CIDR_BLOCK=\"122.64.0.0/16\"  # secondary CIDR block, will be used for pod IPs\nexport AZ1_CIDR=\"122.64.0.0/19\"              # -&gt; make sure to \nexport AZ2_CIDR=\"122.64.32.0/19\"             # -&gt; use the correct\nexport AZ3_CIDR=\"122.64.64.0/19\"             # -&gt; AZ CIDR blocks and masks\nexport AZ1=\"eu-central-1a\"                      \nexport AZ2=\"eu-central-1b\"\nexport AZ3=\"eu-central-1c\"\nexport NODEGROUP_NAME=\"main\"                 # default is 'main', keep this value\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#create-eksdemo-eks-cluster","title":"Create eksdemo EKS cluster","text":"<ul> <li>Use <code>eksdemo</code> to create a PoC EKS cluster.</li> </ul> Create an EKS Cluster with eksdemo or use your own cluster's context<pre><code># this command can take up to 15 minutes to complete\n# change the k8s version if you want to\neksdemo create cluster \"$CLUSTER_NAME\" \\\n    --instance \"m5.large\" \\\n    --nodes 2 \\\n    --version \"1.24\" \\\n    --os \"AmazonLinux2\" \\\n     --vpc-cidr \"$CLUSTER_VPC_CIDR\"\n\n# get the cluster info from eksdemo\neksdemo get cluster \"$CLUSTER_NAME\"\n\n# make sure to use the correct context\neksdemo use-context \"$CLUSTER_NAME\"\n\nkubectl config current-context\n\nkubectl get pod -A\nkubectl get nodes -o wide\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#create-secondary-vpc-cidr","title":"Create Secondary VPC CIDR","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#export-vpc-info","title":"Export VPC Info","text":"<ul> <li>We need to export VPC related info to use in the following steps.</li> </ul> <pre><code>export VPC_ID=$(aws eks describe-cluster --name \"$CLUSTER_NAME\" --query \"cluster.resourcesVpcConfig.vpcId\" --output text)\n\necho \"VPC ID: ${VPC_ID:-'ERROR: should have VPC_ID, fix before continuing'}\"\n\nexport VPC_NAME=$(aws ec2 describe-vpcs --vpc-ids \"$VPC_ID\" --query 'Vpcs[].Tags[?Key==`Name`].Value' --output text)\n\necho \"VPC_NAME: ${VPC_NAME:-'ERROR: should have VPC_NAME, fix before continuing'}\"\n\nexport CLUSTER_SECURITY_GROUP_ID=$(aws eks describe-cluster --name \"$CLUSTER_NAME\" --query cluster.resourcesVpcConfig.clusterSecurityGroupId --output text)\necho \"EKS Cluster($CLUSTER_NAME) has Security Group ID: ${CLUSTER_SECURITY_GROUP_ID:-'ERROR: should have CLUSTER_SECURITY_GROUP_ID, fix before continuing'}\"\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#associate-secondary-cidr-block-to-vpc","title":"Associate Secondary CIDR Block to VPC","text":"<pre><code>echo \"\\nCurrent Subnets:\"\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPC_ID\" \\\n    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \\\n    --output table\n\n# Associate a secondary CIDR block with the cluster VPC\necho \"\\nAssociating secondary CIDR block: $SECONDARY_CIDR_BLOCK with VPC: $VPC_NAME($VPC_ID)\"\naws ec2 associate-vpc-cidr-block --vpc-id \"$VPC_ID\" --cidr-block \"$SECONDARY_CIDR_BLOCK\" --no-cli-pager\n\n\n# see the created associations as a table\necho \"\\n$VPC_NAME($VPC_ID) VPC has associations:\"\naws ec2 describe-vpcs --vpc-ids \"$VPC_ID\" \\\n    --query 'Vpcs[*].CidrBlockAssociationSet[*].{CIDRBlock: CidrBlock, State: CidrBlockState.State}' --out table\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#create-subnets-in-the-secondary-cidr-block","title":"Create Subnets in the Secondary CIDR Block","text":"<p>Create 3 subnets in the Secondary CIDR block with <code>/19</code> mask, so we can have available IP count of <code>3*8190</code> or <code>24570</code> for our pods.</p> Set CUSTOM_SNET{1,2,3} variables if you've already created the subnets<pre><code># Control the variables before creating the subnets\necho \"Provided Vars (for later export):\\nexport AZ1=$AZ1\\nexport AZ2=$AZ2\\nexport AZ3=$AZ3\\nexport AZ1_CIDR=$AZ1_CIDR\\nexport AZ2_CIDR=$AZ2_CIDR\\nexport AZ3_CIDR=$AZ3_CIDR\\n\"\n\n# create the subnets\necho \"Creating Subnets on AZs: $AZ1, $AZ2 and $AZ3\"\nexport CUST_SNET1=$(aws ec2 create-subnet --cidr-block \"$AZ1_CIDR\" --vpc-id \"$VPC_ID\" --availability-zone \"$AZ1\" | jq -r .Subnet.SubnetId)\nexport CUST_SNET2=$(aws ec2 create-subnet --cidr-block \"$AZ2_CIDR\" --vpc-id \"$VPC_ID\" --availability-zone \"$AZ2\" | jq -r .Subnet.SubnetId)\nexport CUST_SNET3=$(aws ec2 create-subnet --cidr-block \"$AZ3_CIDR\" --vpc-id \"$VPC_ID\" --availability-zone \"$AZ3\" | jq -r .Subnet.SubnetId)\n\necho -e \"Created Subnets:\\nexport CUST_SNET1=$CUST_SNET1 # ($AZ1_CIDR)\\nexport CUST_SNET2=$CUST_SNET2 # ($AZ2_CIDR)\\nexport CUST_SNET3=$CUST_SNET3 # ($AZ3_CIDR)\"\n# do this if associated to Public Route table ( or to an internet gateway)\n# Enable auto-assign public IPv4 addresses\n# aws ec2 modify-subnet-attribute --subnet-id \"$CUST_SNET1\" --map-public-ip-on-launch \n# aws ec2 modify-subnet-attribute --subnet-id \"$CUST_SNET2\" --map-public-ip-on-launch \n# aws ec2 modify-subnet-attribute --subnet-id \"$CUST_SNET3\" --map-public-ip-on-launch \n</code></pre> <pre><code>echo \"VPC Subnets after Secondary CIDR has been populated with 3 subnets:\"\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPC_ID\" \\\n    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \\\n    --output table\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#route-table-configuration","title":"Route Table Configuration","text":"<ul> <li>Our subnets must have an egress to an <code>InternetGateway</code> or a <code>NatGateway</code>, so we need to configure the route tables.</li> <li>We will use the auto-generated Route Table for our Secondary CIDR block, and we will add a route to the NAT Gateway.</li> </ul> You can skip this step if you handled the route table configuration manually<pre><code># Find the RouteTableID of the main route table\nexport MAIN_ROUTE_TABLE_ID=$(aws ec2 describe-route-tables \\\n    --filters \"Name=route.destination-cidr-block,Values=${SECONDARY_CIDR_BLOCK}\"  \"Name=association.main,Values=true\" \\\n    --query 'RouteTables[].RouteTableId' --output text)\n\necho \"Routes of RouteTable w/ ID: ${MAIN_ROUTE_TABLE_ID:-'MAIN_ROUTE_TABLE_ID variable should have a value of route-table-id, fix before continuing...'}\"\n\naws ec2 describe-route-tables --route-table-ids \"$MAIN_ROUTE_TABLE_ID\" --query 'RouteTables[].Routes[]' --output table\n\n\n# not really needed, bc aws is smart enough to group our secondary CIDR subnets in a route table\n#   aws ec2 associate-route-table --route-table-id $MAIN_ROUTE_TABLE_ID --subnet-id $CUST_SNET1\n#   aws ec2 associate-route-table --route-table-id $MAIN_ROUTE_TABLE_ID --subnet-id $CUST_SNET2\n#   aws ec2 associate-route-table --route-table-id $MAIN_ROUTE_TABLE_ID --subnet-id $CUST_SNET3\n\n# Find a (the first) NATGW ID in the VPC\nexport FIRST_NATGW_ID=$(aws ec2 describe-nat-gateways \\\n    --filter \"Name=vpc-id,Values=${VPC_ID}\" \\\n    --query 'NatGateways[0].NatGatewayId' --output text 2&gt;&amp;1)\n\necho \"Found (first) NATGW ID: ${FIRST_NATGW_ID:-'FIRST_NATGW_ID variable should have a value, fix before continuing...'}\"\n</code></pre> <pre><code># create a route in the MAIN_ROUTE_TABLE_ID and at 0.0.0.0/0 with the FIRST_NATGW_ID. Description: \"Route to secondary CIDR block\"\necho \"Creating NATGW route in RouteTable(${MAIN_ROUTE_TABLE_ID})\"\nnatgw_route_has_created=$(aws ec2 create-route \\\n    --route-table-id \"$MAIN_ROUTE_TABLE_ID\" \\\n    --destination-cidr-block \"0.0.0.0/0\" \\\n    --nat-gateway-id \"$FIRST_NATGW_ID\" --output text)\necho \"In RouteTable(${MAIN_ROUTE_TABLE_ID}), created a route at 0.0.0.0/0 to NATGW(${FIRST_NATGW_ID}): $natgw_route_has_created\"\n\naws ec2 describe-route-tables --route-table-ids \"$MAIN_ROUTE_TABLE_ID\" --query 'RouteTables[].Routes[]' --output table\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#tagging-the-resources-properly","title":"Tagging the Resources properly","text":"<ul> <li><code>Karpenter AWSNodeTemplate</code> objects select the Subnets and Security Groups based on the <code>karpenter.sh/discovery</code> tag.     A fragment of a Karpenter AWSNodeTemplate yaml<pre><code>kind: AWSNodeTemplate\nspec:\n    subnetSelector: \n        karpenter.sh/discovery: \"${CLUSTER_NAME}\"        \n    securityGroupSelector: \n        karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n</code></pre></li> </ul> Find the NodeGroup Subnets and tag them: karpenter.sh/discovery=${CLUSTER_NAME}<pre><code># We need to keep the original EKS Node Group configuration\n# regarding subnets. Our configuration tries to keep Nodes and Pods\n# in different CIDR blocks, so we need to tag the existing subnets.\nexisting_node_group_subnets=$(aws eks describe-nodegroup \\\n  --cluster-name \"${CLUSTER_NAME}\" \\\n  --nodegroup-name \"${NODEGROUP_NAME}\" \\\n  --query 'nodegroup.subnets' \\\n  --output text | cat -s \\\n  | awk -F'\\t' '{for (i = 1; i &lt;= NF; i++) print $i}')\n\necho \"Existing Node Group Subnets: \\n${existing_node_group_subnets:-'ERROR: should have existing_node_group_subnets, fix before continuing'}\"\n</code></pre> <ul> <li>The tags that start with <code>kubernetes.io/*</code> are required.</li> <li>For <code>kubernetes.io/role/*</code> tag, follow the below rules:<ul> <li>Public Subnets: <code>kubernetes.io/role/elb,Value=1</code></li> <li>Private Subnets: <code>kubernetes.io/role/internal-elb,Value=1</code></li> </ul> </li> </ul> Do the actual tagging after you've checked the output of the previous command<pre><code>while IFS=$'\\t' read -r subnet_id ; do\n    # echo \"${subnet_id}\"\n    echo \"Tagging Subnet: $subnet_id of EKS Cluster: $CLUSTER_NAME + NodeGroup: $NODEGROUP_NAME\"\n    aws ec2 create-tags --resources \"$subnet_id\" --tags \\\n    \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\"\ndone &lt;&lt;&lt; $existing_node_group_subnets\n</code></pre> Tag the Subnets we created for the Secondary CIDR block + Cluster SG<pre><code># tag the subnets\naws ec2 create-tags --resources \"$CUST_SNET1\" --tags \\\n    \"Key=Name,Value=SecondarySubnet-A-${CLUSTER_NAME}\" \\\n    \"Key=kubernetes.io/role/internal-elb,Value=1\" \\\n    \"Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared\"\n\naws ec2 create-tags --resources \"$CUST_SNET2\" --tags \\\n    \"Key=Name,Value=SecondarySubnet-B-${CLUSTER_NAME}\" \\\n    \"Key=kubernetes.io/role/internal-elb,Value=1\" \\\n    \"Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared\"\n\naws ec2 create-tags --resources \"$CUST_SNET3\" --tags \\\n    \"Key=Name,Value=SecondarySubnet-C-${CLUSTER_NAME}\" \\\n    \"Key=kubernetes.io/role/internal-elb,Value=1\" \\\n    \"Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared\"\n\n# tag Cluster Security Group as well \n# (NOTE: the tag \"kubernetes.io/cluster/${CLUSTER_NAME}=shared\" is required and is probably already there)\naws ec2 create-tags --resources \"$CLUSTER_SECURITY_GROUP_ID\" --tags \\\n    \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\" \\\n    \"Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=owned\"\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/1-vpc-secondary-cidr-and-subnets/#next-steps","title":"Next Steps","text":"<ul> <li>2. AWS VPC CNI &amp; ENIConfig configuration for Custom Networking</li> <li>3. Karpenter v1alpha Configuration (Provider &amp; AWSNodeTemplate)</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/","title":"2. AWS VPC CNI &amp; ENIConfig configuration for Custom Networking","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#aws-node-and-cni-configuration","title":"AWS-Node and CNI Configuration","text":"<pre><code># Get the current env vars of aws-node\nkubectl get daemonset aws-node -n kube-system \\\n  -o jsonpath='{.spec.template.spec.containers[0].env}' \\\n  | jq -r '.[] | .name + \"=\" + .value'\n\n# Enable custom network config in aws-node\nkubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true\n\nkubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone\n\n# restart aws eni controller\nkubectl rollout restart ds/aws-node -n kube-system\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#environment-variables-explained","title":"Environment Variables Explained","text":"<ul> <li><code>kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone</code>:</li> <li>Means that AWS CNI is using the topology.kubernetes.io/zone label to determine the <code>ENIConfig</code> name(<code>kubectl get eniconfig</code>) for that node.</li> <li>topology.kubernetes.io/zone label is automatically added to the nodes by the kubelet as <code>eu-west-1a</code> or <code>eu-west-1b</code> or <code>eu-west-1c</code>, so we don't need any extra node tagging to do.</li> <li>This way we have a consistent way of applying the ENIConfig to the nodes.</li> <li><code>ENIConfig</code> has the info about which Subnet and Security Groups should be used for the ENI.</li> <li>Our nodes will have their 1st ENI configured with the default VPC CIDR block, and the 2nd ENI will be configured with the Secondary CIDR block.</li> <li>Pods get their IPs from 2nd ENI, and the 1st ENI is used for the node communication.</li> <li>We will have 1st ENI reserved for Nodes, and all other ENIs will be used for the pod communication and in the Secondary CIDR block.</li> <li><code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true</code>:</li> <li>Enables the Custom Network Configuration for AWS CNI. This change will help us to use the secondary CIDR block for the pods.</li> <li>AWS CNI will use the <code>ENIConfig</code> objects to create and configure the ENIs.</li> <li>AWS CNI will look for the label <code>${ENI_CONFIG_LABEL_DEF}</code> on the node, and will use the value of that label to find the <code>ENIConfig</code> object by name.</li> <li>This configuration requires the existing node EC2 Instances be be restarted to take effect.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#more-on-eniconfig-crds","title":"More on ENIConfig CRDs","text":"<ul> <li>ENIConfig CRD is used by AWS CNI to create ENIs with the specified configuration for that Availability Zone.</li> <li>The deamonset <code>aws-node</code> has env. var. called <code>ENI_CONFIG_LABEL_DEF</code>, and it is used to match</li> </ul> <pre><code>NodeLabels:\n    topology.kubernetes.io/zone=eu-west-1a\n    ...\n\nAWS CNI makes the following configuration\n    (selected ENIConfig name for node) = NodeLabels[ENI_CONFIG_LABEL_DEF]\n</code></pre> <ul> <li>We are informing AWS CNI to look for the node label <code>topology.kubernetes.io/zone</code>.</li> <li>For example, if the label value is <code>eu-west-1a</code>, AWS CNI will use the <code>ENIConfig</code> named <code>eu-west-1a</code>.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#lets-create-the-eniconfig-objects","title":"Let's create the ENIConfig objects","text":"<ul> <li>It's crucial to use AZ name as the ENIConfig name.</li> <li>This is because the <code>aws-node</code> deamonset uses the <code>ENI_CONFIG_LABEL_DEF</code> env. var. to match the node label value to the <code>ENIConfig</code> name.</li> </ul> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n name: $AZ1\nspec:\n  securityGroups: [\"$CLUSTER_SECURITY_GROUP_ID\"]\n  subnet: \"$CUST_SNET1\"\nEOF\n\ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n name: $AZ2\nspec:\n  securityGroups: [\"$CLUSTER_SECURITY_GROUP_ID\"]\n  subnet: \"$CUST_SNET2\"\nEOF\n\ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n name: $AZ3\nspec:\n  securityGroups: [\"$CLUSTER_SECURITY_GROUP_ID\"]\n  subnet: \"$CUST_SNET3\"\nEOF\n\nkubectl get eniconfig ${AZ1} -o yaml; echo \"---\";\nkubectl get eniconfig ${AZ2} -o yaml; echo \"---\";\nkubectl get eniconfig ${AZ3} -o yaml; echo \"---\";\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#restart-the-node-group-instances","title":"Restart the Node Group Instances","text":"<ul> <li>Terminate the Node Group instances to have them recreated with the new ENI configuration.</li> <li>Node should get it's primary ENI from the default VPC CIDR block, and the secondary ENI(and any other ENIs) from the Secondary CIDR block.</li> <li>This is because we have configured the ENIConfig objects to use the Secondary CIDR block subnets.</li> <li>After you recreate the Node instances, check the instances to see if they got their IP Addresses from VPC Secondary CIDR Block</li> <li>You should be seeing 1st ENI with IP from the default VPC CIDR block, and others from the Secondary CIDR block</li> <li></li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#test-pods-having-ip-addresses-from-secondary-cidr-block","title":"Test Pods having IP addresses from Secondary CIDR Block","text":"<pre><code>kubectl create deployment nginx --image=nginx\nkubectl scale --replicas=5 deployments/nginx\nkubectl expose deployment/nginx --type=NodePort --port 80\n\n# try to see if the pods are running on the secondary CIDR block \n# (note: ignore daemonset pods)\nkubectl get pods -o wide\n\nkubectl port-forward svc/nginx 8000:80\n# check localhost:8000 on browser\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/2-aws-vpc-cni-configuration/#next-steps","title":"Next Steps","text":"<ul> <li>3. Karpenter v1alpha Configuration (Provider &amp; AWSNodeTemplate)</li> <li>Create a load on the cluster to trigger Karpenter</li> <li>Demo Troubleshooting</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/","title":"3. Karpenter <code>v1alpha</code> Configuration (Provider &amp; AWSNodeTemplate)","text":"<ul> <li>This configuration is for Karpenter <code>v1alpha</code> versions (or <code>v0.31.0</code> and below).</li> <li>Check out: Migrate from AWSNodeTemplate to NodeClass if you wish to upgrade <code>v0.32.0</code> and above.</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#install-karpenter","title":"Install Karpenter","text":"<pre><code># NOTE: if you're planning to use Karpenter v1alpha and below, \n#       you should use an option to specify Karpenter version &lt;= v0.31\n#       in this eksdemo command\n\neksdemo install autoscaling-karpenter \\\n    --cluster \"$CLUSTER_NAME\" \\\n    --set \"hostNetwork=true\"\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#karpenter-configuration","title":"Karpenter Configuration","text":"<ul> <li>Let's delete the default karpenter config (note: important to delete them beforehand)</li> </ul> <pre><code>kubectl -n karpenter get provisioners default -o yaml\nkubectl -n karpenter get awsnodetemplate default -o yaml\n\n# delete default ones\nkubectl delete provisioners default\nkubectl delete awsnodetemplate default\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#create-karpenter-provisioner","title":"Create Karpenter Provisioner","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: default\nspec:\n  ttlSecondsUntilExpired: 604800 # expire nodes after 7 days (in seconds) = 7 * 60 * 60 * 24\n  providerRef:\n    name: default\n  consolidation:\n    enabled: true\n  limits:\n    resources:\n      cpu: 1k\n  requirements:\n    # Include general purpose instance families\n    - key: karpenter.k8s.aws/instance-family\n      operator: In\n      values: [c5, m5, r5]\n    - key: karpenter.sh/capacity-type\n      operator: In\n      values:\n      - on-demand\n      - spot\n    - key: kubernetes.io/arch\n      operator: In\n      values: [amd64]\n  # Karpenter provides the ability to specify a few additional Kubelet args.\n  # These are all optional and provide support for additional customization and use cases.\n  kubeletConfiguration:\n    maxPods: 20\nEOF\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#create-karpenter-awsnodetemplate","title":"Create Karpenter AWSNodeTemplate","text":"<p>-</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: default\nspec:\n  subnetSelector:\n    # should be the primary cidr block subnets\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n  securityGroupSelector:\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n  amiFamily: \"AL2\"\n  userData: |\n    #!/bin/bash\n    echo \"Running custom user data script\"\n    sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n    sudo systemctl status amazon-ssm-agent\n  tags:\n    dev.corp.net/app: ExampleTag\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n    Name: \"Karpenter-Node-${CLUSTER_NAME}\"\n  # optional, configures storage devices for the instance\n  blockDeviceMappings: # AL2 machine defaults\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        encrypted: true\n  # optional, configures detailed monitoring for the instance\n  # detailedMonitoring: \"...\"\nEOF\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#check-the-status-of-the-karpenter-awsnodetemplate","title":"Check the <code>.status</code> of the Karpenter AWSNodeTemplate","text":"<ul> <li><code>AWSNodeTemplate</code> object will update it's <code>.status</code> definition with the resolved Subnet and Security Group IDs.</li> <li>Check these IDs to make sure they are correct.</li> </ul> <pre><code>kubectl -n karpenter get awsnodetemplate default -o yaml | yq '.status'\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/3-karpenter-v1alpha-configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Create a load on the cluster to trigger Karpenter</li> <li>Demo Troubleshooting</li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/create-load-on-the-cluster/","title":"Create Load on the Cluster","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/create-load-on-the-cluster/#autoscaling-nodes-with-karpenter","title":"Autoscaling Nodes with Karpenter","text":"<pre><code># ======== CREATE A LOAD\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate-netshoot\n  template:\n    metadata:\n      labels:\n        app: inflate-netshoot\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: inflate-netshoot\n          image: nicolaka/netshoot\n          command: [\"/bin/bash\"]\n          args: [\"-c\", \"while true; do ping localhost; sleep 60;done\"]\n          resources:\n            requests:\n              cpu: 1\nEOF\n\n\nkubectl scale deployment inflate --replicas 4\n\nkubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller\nkubectl get nodes\n\nkubectl exec inflate-netshoot-xxxx-yyy -- nslookup kubernetes.default\n\nkubectl exec -it inflate-netshoot-xxxx-yyy -- /bin/bash\n\n\n# ======== SCALE DOWN\nkubectl scale deployment inflate --replicas 0\n\n# check karpenter logs\nkubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller\n\nkubectl get nodes\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#find-which-resources-are-tagged-with-karpentershdiscoverycluster_name","title":"Find which resources are tagged with <code>karpenter.sh/discovery=${CLUSTER_NAME}</code>","text":"<pre><code># List all resources with the tag: Key=karpenter.sh/discovery,Values=${CLUSTER_NAME}\naws resourcegroupstaggingapi get-resources \\\n    --tag-filters \"Key=karpenter.sh/discovery,Values=${CLUSTER_NAME}\" \\\n    --query 'ResourceTagMappingList[]' --output text \\\n    | sed 's/arn:/\\n----------\\narn:/g'\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#helpful-bash-functions","title":"Helpful bash functions","text":"<pre><code>alias klogs_karpenter=\"kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter\"\nalias klogs_coredns=\"kubectl logs -f -n kube-system deploy/coredns\"\nalias klogs_aws_node=\"kubectl logs -f -n kube-system -l k8s-app=aws-node\"\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#debug-cluster-dns","title":"Debug Cluster DNS","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: default\nspec:\n  containers:\n  - name: dnsutils\n    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\n    command:\n      - sleep\n      - \"infinity\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n  # nodeName: ip-105-64-46-249.eu-central-1.compute.internal\nEOF\n\n# try to resolve cluster DNS from the pod\nkubectl exec dnsutils -- nslookup kube-dns.kube-system\nkubectl exec dnsutils -- nslookup kubernetes.default\nkubectl exec dnsutils -- nslookup google.com\n\n# if karpenter is installed\nkubectl exec dnsutils -- nslookup karpenter.karpenter.svc\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#test-pods-having-ip-addresses-from-secondary-cidr-block","title":"Test Pods having IP addresses from Secondary CIDR Block","text":"<pre><code>kubectl create deployment nginx --image=nginx\nkubectl scale --replicas=3 deployments/nginx\nkubectl expose deployment/nginx --type=NodePort --port 80\n\n\nkubectl port-forward svc/nginx 9090:80\n# check localhost:9090 on browser\n\n# try to see if the pods are running on the secondary CIDR block\n# (p.s. ignore daemonset pods or hostNetwork:true pods)\nkubectl get pods -o wide\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#aws-cli-ssm-session-manager","title":"AWS CLI SSM Session Manager","text":"<ul> <li>Install AWS CLI SSM Session Manager Plugin</li> <li>EC2 instance must have SSM Agent installed (possibly in userdata)</li> <li>Connect to EC2 instance via SSM Session Manager, or you can use the AWS Console UI.   <pre><code># you can SSH into the Karpenter nodes like this\naws ssm start-session --target i-061f1a56dfff5d8f3\n</code></pre></li> </ul>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#error-address-is-not-allowed","title":"Error: Address is not allowed","text":"<ul> <li>You can get the following error if you forget to set <code>hostNetwork: true</code> in the Karpenter deployment.</li> </ul> <pre><code>Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: failed calling webhook \"defaulting.webhook.karpenter.k8s.aws\": failed to call webhook: Post \"https://karpenter.karpenter.svc:8443/default/karpenter.k8s.aws?timeout=10s\": Address is not allowed\n</code></pre>"},{"location":"eksdemo-secondary-cidr-and-cni-custom-netwoking/troubleshooting/#eks-health-issues","title":"EKS Health Issues","text":"<ul> <li>You may get this error if you route the Subnets on Secondary CIDR to a Internet Gateway, making it a public subnet.</li> <li>If this is the case, you must enable auto-assign public IP address for the subnet.</li> </ul> <pre><code>Ec2SubnetInvalidConfiguration\n    One or more Amazon EC2 Subnets of [subnet-00782ed1060ae5f88, subnet-0af9794264f7165bc, subnet-0b974d5872910ab7b] for node group mymymy does not automatically assign public IP addresses to instances launched into it. If you want your instances to be assigned a public IP address, then you need to enable auto-assign public IP address for the subnet. See IP addressing in VPC guide: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-ip-addressing.html#subnet-public-ip\n</code></pre>"}]}